---
title: "Prediction Analysis"
author: "Malick Lingani"
date: "Sunday, November 23, 2014"
output: html_document
---
  
#1. Executive Summary
The goal this project is to predict the manner in which people do they exercise. For this purpose, data was collected by a group of enthusiasts who use devices such as Jawbonetake to take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks.
  
Plese find more about the data at [Human Activity Recognition - HAR](http://groupware.les.inf.puc-rio.br/har). We keep the data downloaded on this day **`r Sys.Date()`** for reproduicibility purposes in the folder "Original_Data".  
  
We are using the "classe" variable in the training set as outcome. The other variables are used as predictors. But before making prediction analysis we perform some preprocessing work to reduce the number of predictors.
  
Cross validation, Tree and Rpart model fitting analysis are performed. Redom Forests and Out-of Sample Accuracy are also perfoirmed.
  
As Conclusion we find that the random forest model is making good in prediction the "classe" variable.
  
#2. Data loading and Preprocessing
##2.1. Loading Data.  
if you want to reproduce the analysis please considere changing the working directory and destination directories for downloaded data.
  
```{r Downloading_Data}
setwd("C:/Users/Malickma/Documents/GitHub/PML-Prediction-Assignment")
#Downloading data
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
              destfile = "Original_Data/pml-training.csv")
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
              destfile = "Original_Data/pml-testing.csv")
#Reading data
pml_training = read.csv("Original_Data/pml-training.csv", na.strings=c("", "NA", "NULL"))
pml_testing = read.csv("Original_Data/pml-testing.csv", na.strings=c("", "NA", "NULL"))
#dim(pml_training); dim(pml_testing)
```  
Training data has `r dim(pml_training)[1]` rows and `r dim(pml_training)[2]` columns.  
Testing data has `r dim(pml_testing)[1]` rows and `r dim(pml_testing)[2]` columns
  
##2.2. Preprocessing Data
For the purposes of the prediction analysis we will try to reduice the number of predictors through the 4 following steps bellow.
  
**1. Dealing with NAs**  
First, variables that have too many NAs are likely to be unrelevant
```{r Remove_NAs}
pml_training1 <- pml_training[ , colSums(is.na(pml_training)) == 0]
```
  
**2. Dealing with unrelevant  variables**  
Second, there are also variables ('X', 'user_name', 'raw_timestamp_part_1', ...) that are unlikelly to be related to dependent variables.#3. Analysis
```{r Remove_Unrelated}
unrelevant  = c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2',
              'cvtd_timestamp', 'new_window', 'num_window')
pml_training2 <- pml_training1[, -which(names(pml_training1) %in% unrelevant )]
```
  
**3. Dealing with extremly low variance variables**  
Check the variables that have extremely low variance (this method is useful nearZeroVar() ). Those variables are unlikely to influence the outcome.
```{r Remove_lowVar}
library(caret, verbose = FALSE)
# only numeric variabls can be evaluated in this way.
lowVar= nearZeroVar(pml_training2[sapply(pml_training2, is.numeric)], saveMetrics = TRUE)
pml_training3 = pml_training2[,lowVar[, 'nzv']==0]
dim(pml_training3)

```
  
**4. Dealing with highly correlated variables**  
Remove highly correlated variables 90% (using for example findCorrelation() ). Variables that are quite identical to the outcome variable are removed.
```{r Remove_hightCorr1}
# only numeric variabls can be evaluated in this way.
corrMatrix <- cor(na.omit(pml_training3[sapply(pml_training3, is.numeric)]))
dim(corrMatrix)
# there are 52 variables.
corrDF <- expand.grid(row = 1:52, col = 1:52)
corrDF$correlation <- as.vector(corrMatrix)
levelplot(correlation ~ row+ col, corrDF)
```
  
We are going to remove those variable which have high correlation.
```{r Remove_hightCorr2}
highCorr <- findCorrelation(corrMatrix, cutoff = .90, verbose = FALSE)
pml_training4 <- pml_training3[,-highCorr]
#dim(pml_training4)
```
  
The 4 preprocessing steps above reduced the number of variables from `r dim(pml_training)[2]` to `r dim(pml_training4)[2]`.


#3. Model Fitting and Cross Validation
##3.1 Split data to training and testing for cross validation.
```{r Split}
inTrain <- createDataPartition(y=pml_training4$classe, p=0.7, list=FALSE)
training <- pml_training4[inTrain,]; testing <- pml_training4[-inTrain,]
#dim(training);dim(testing)
```
We got `r dim(training)[1]` Training sample and `r dim(testing)[1]` Testing sample.
  
##3.1 Model Fiting with Regression Tree
With our processed data we will fit the model with Regression Tree form Tree package.
```{r tree}
library(tree)
set.seed(12345)
tree.training=tree(classe~.,data=training)
summary(tree.training)
```
Ploting the Tree  
```{r tree_plot }
plot(tree.training, asp = 1/3)
text(tree.training,pretty=0, all=TRUE,cex =.5)
```
  
We get a so bushy tree, and we are going to prune it to reduce number of nodes.
  
##3.2 Model Fiting with Rpart
With our processed data we will fit the model with Rpart method form Caret package.
```{r caret_tree}
library(caret, verbose=FALSE)
modFit <- train(classe ~ .,method="rpart",data=training)
print(modFit$finalModel)
```

ploting the fitted model with rattle package
```{r }
library(rattle, verbose=FALSE)
fancyRpartPlot(modFit$finalModel)

```
  
##3.3 Cross Validation
We are going to check the performance of the tree on the testing data by cross validation.
```{r Cross_Val_tree}
tree.pred=predict(tree.training,testing,type="class")
predMatrix = with(testing,table(tree.pred,classe))
sum(diag(predMatrix))/sum(as.vector(predMatrix)) # error rate


```
The 0.667 is not very accurate.
  
```{r Cross_Val_mpart}
tree.pred=predict(modFit,testing)
predMatrix = with(testing,table(tree.pred,classe))
sum(diag(predMatrix))/sum(as.vector(predMatrix)) # error rate
```
The 0.50 from 'caret' package is much lower than the result from 'tree' package.
  
##3.4 Pruning tree
This tree was grown to full depth, and might be too variable. We now use Cross Validation to prune it.
```{r }
cv.training=cv.tree(tree.training,FUN=prune.misclass)
cv.training
```
ploting for better visualization  
```{r pruning_plot}
plot(cv.training)
```
  
It shows that when the size of the tree goes down, the deviance goes up. It means the 20 is a good size (i.e. number of terminal nodes) for this tree. We do not need to prune it.

Suppose we prune it at size of nodes at 18.
```{r nodes18}
prune.training=prune.misclass(tree.training,best=18)
```  

Now lets evaluate this pruned tree on the test data.
```{r }
tree.pred=predict(prune.training,testing,type="class")
predMatrix = with(testing,table(tree.pred,classe))
sum(diag(predMatrix))/sum(as.vector(predMatrix)) # error rate
```
0.662 is a little less than 0.667, so pruning did not hurt us with repect to misclassification errors, and gave us a simpler tree. We use less predictors to get almost the same result. By pruning, we got a shallower tree, which is easier to interpret.

The single tree is not good enough, so we are going to use bootstrap to improve the accuracy. We are going to try random forests.

##3.5 Random Forests 
These methods use trees as building blocks to build more complex models. 
Random forests build lots of bushy trees, and then average them to reduce the variance.  
Lets fit a random forest and see how well it performs.  
```{r rf}
require(randomForest)
set.seed(12345)
rf.training=randomForest(classe~.,data=training,ntree=100, importance=TRUE)
rf.training
```
  
Visualizing Variables Importance
```{r impVarPlot}
varImpPlot(rf.training,)
```
  
we can see which variables have higher impact on the prediction.
  
#4. Out-of Sample Accuracy
Our Random Forest model shows OOB estimate of error rate: 0.72% for the training data. Now we will predict it for out-of sample accuracy.  
Now lets evaluate this tree on the test data.
```{r OOS_acc}
tree.pred=predict(rf.training,testing,type="class")
predMatrix = with(testing,table(tree.pred,classe))
sum(diag(predMatrix))/sum(as.vector(predMatrix)) # error rate
```
  
0.99 means we got a very accurate estimate.  
No. of variables tried at each split: 6. It means every time we only randomly use 6 predictors to grow the tree. Since p = 43, we can have it from 1 to 43, but it seems 6 is enough to get the good result.


#5. Conclusion
Now we can predict the testing data from the website.
```{r predicting}
answers <- predict(rf.training, pml_testing)
answers
```
Those answers are going to submit to website for grading. It shows that this random forest model did a good job.
  
here is the function used to generate the result files for submission.
  
```{r subm_files}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("Preduction_Results/", "problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(answers)
```